# Machine Learning Concepts
To avoid overfiting (linked to training data), we use:

# Regularization L1
throws information that is not important to 0, thus definitely excluding the coefficients.
# Regularization L2
makes it tend to zero, trying as little as possible. Always try to keep the value (ex: 0.00001)


When we have several data with different scales, we perform a normalization.
Like the varied forms, one can perform a simple as follows:
- we take the lowest value
- we got the biggest
- and even a box of these data from 0 to 1

Dependent and independent variables
Km / l (dep / indep)

A = W * X + B

A-> exit
W-> weight
X-> parameter
B-> bias or vies